{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f197292e-c995-45c5-ab44-9b427c36cb0c",
   "metadata": {},
   "source": [
    "# REDCap Data Synthesiser\n",
    "\n",
    "This program retrieves a data dictionary from a REDCap project, or from a supplied Data Dictionary CSV file, and builds a data synthesis model, which is then applied for `num_records` records, to generate realistic looking, completely synthetic data, and optionally output the records as JSON, export as CSV, or, if the project is not in production, import into the project via the API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1303397b-0bdf-43c0-9d0d-68e14208b58b",
   "metadata": {},
   "source": [
    "Execute the below cell to generate the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a2a30a-008e-4e04-bf5d-6b8b4a9a9050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faker\n",
    "import random\n",
    "import json\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "# Initialize Faker\n",
    "fake = faker.Faker()\n",
    "\n",
    "# retrieve metadata\n",
    "def retrieve_metadata(api_url, token):\n",
    "    data = {\n",
    "        'token': token,\n",
    "        'content': 'metadata',\n",
    "        'format': 'json',\n",
    "        'returnFormat': 'json'\n",
    "    }\n",
    "    r = requests.post(api_url,data=data)\n",
    "    if r.status_code == 200:\n",
    "        # Parse the JSON response\n",
    "        json_data = r.json()\n",
    "\n",
    "        # Define a list to store parsed data\n",
    "        parsed_data = []\n",
    "\n",
    "        # Iterate over each field in the JSON data\n",
    "        for field in json_data:\n",
    "            parsed_field = {\n",
    "                'field_name': field['field_name'],\n",
    "                'field_label': field['field_label'],\n",
    "                'field_type': field['field_type'],\n",
    "                'select_choices_or_calculations': field['select_choices_or_calculations'],\n",
    "                'text_validation_type_or_show_slider_number': field['text_validation_type_or_show_slider_number'],\n",
    "                'text_validation_min': field['text_validation_min'],\n",
    "                'text_validation_max': field['text_validation_max']\n",
    "            }\n",
    "            # Append the parsed field to the list\n",
    "            parsed_data.append(parsed_field)\n",
    "\n",
    "        # return the parsed data\n",
    "        #print(parsed_data)  # For troubleshooting\n",
    "        return parsed_data\n",
    "      \n",
    "    else:\n",
    "        raise ValueError('Error: HTTP Status ' + str(r.status_code))\n",
    "\n",
    "\n",
    "def preprocess_data_dictionary(data_dictionary):\n",
    "    # Preprocess the data dictionary and create a mapping of field names to functions\n",
    "    field_mapping = {}\n",
    "    for field in data_dictionary:\n",
    "        field_name = field['field_name']\n",
    "        field_type = field['field_type']\n",
    "        # Add logic to map field names to functions based on field type, validation type, etc.\n",
    "        if field_type == 'text':\n",
    "            field_mapping[field_name] = generate_text_field_value\n",
    "        elif field_type in ['yesno', 'truefalse']:\n",
    "            field_mapping[field_name] = generate_boolean_field_value\n",
    "        elif field_type in ['radio', 'dropdown']:\n",
    "            field_mapping[field_name] = generate_radio_dropdown_field_value\n",
    "        elif field_type == 'checkbox':\n",
    "            field_mapping[field_name] = generate_checkbox_field_value    \n",
    "        elif field_type == 'slider':\n",
    "            field_mapping[field_name] = generate_slider_field_value\n",
    "        elif field_type == 'notes':\n",
    "            field_mapping[field_name] = generate_notes_field_value\n",
    "        # Add more field type mappings as needed\n",
    "    return field_mapping\n",
    "\n",
    "def generate_text_field_value(field):\n",
    "    # Generate synthetic data for text fields\n",
    "    field_name = field['field_name']\n",
    "    field_label = field['field_label'].lower()\n",
    "    validation_type = field['text_validation_type_or_show_slider_number']\n",
    "    min_val = field['text_validation_min']\n",
    "    max_val = field['text_validation_max']\n",
    "    \n",
    "    # date validated fields\n",
    "    if validation_type in ['date_dmy','date_mdy','date_ymd','datetime_dmy','datetime_mdy','datetime_seconds_dmy','datetime_seconds_mdy','datetime_seconds_ymd','datetime_ymd']:\n",
    "        date_format_mapping = {\n",
    "            'date_dmy': '%Y-%m-%d',\n",
    "            'date_mdy': '%Y-%m-%d',\n",
    "            'date_ymd': '%Y-%m-%d',\n",
    "            'datetime_dmy': '%Y-%m-%d %H:%M',\n",
    "            'datetime_mdy': '%Y-%m-%d %H:%M',\n",
    "            'datetime_seconds_dmy': '%Y-%m-%d %H:%M:%S',\n",
    "            'datetime_seconds_mdy': '%Y-%m-%d %H:%M:%S',\n",
    "            'datetime_seconds_ymd': '%Y-%m-%d %H:%M:%S',\n",
    "            'datetime_ymd': '%Y-%m-%d %H:%M',\n",
    "        }\n",
    "        strftime_param = date_format_mapping[validation_type]\n",
    "        if (field_label == 'dob') or ('birth' in field_label) or (field_name == 'dob') or ('birth' in field_name):\n",
    "            return fake.date_of_birth().strftime(strftime_param)\n",
    "        else:\n",
    "            return fake.date_time_this_century().strftime(strftime_param)\n",
    "        \n",
    "    # integer and number\n",
    "    elif validation_type in ['integer', 'number']:\n",
    "        field_label = field['field_label'].lower()\n",
    "        # Handle integer and number types\n",
    "        if 'height' in field_label:\n",
    "            # Generate height around 175 with standard deviation 6.5\n",
    "            return round(random.normalvariate(175, 6.5), 2)\n",
    "        elif 'weight' in field_label:\n",
    "            # Generate weight around 63 with standard deviation 10\n",
    "            return round(random.normalvariate(63, 10), 2)\n",
    "        else:\n",
    "            return fake.random_int(min=int(min_val), max=int(max_val))\n",
    "    \n",
    "    # Email\n",
    "    elif validation_type == 'email':\n",
    "        # Handle email type\n",
    "        return f\"{fake.first_name()}.{fake.last_name()}@{email_domain}\"\n",
    "    \n",
    "    # Everything else. Could anticipate more types below. Could even guess based on field label and name, and apply the closest faker library method.\n",
    "    else:\n",
    "        # Handle other text types\n",
    "        if any(word in field_label for word in ['first name', 'fname', 'personal name', 'given name', 'christian name']):\n",
    "            return fake.first_name()\n",
    "        elif any(word in field_label for word in ['last name', 'family name', 'lname', 'surname']):\n",
    "            return fake.last_name()\n",
    "        else:\n",
    "            return fake.word()\n",
    "        \n",
    "def generate_notes_field_value(field):\n",
    "    # Generate synthetic data for notes fields\n",
    "    return fake.text()\n",
    "\n",
    "def generate_boolean_field_value(field):\n",
    "    # Generate synthetic data for boolean fields (yes/no or true/false)\n",
    "    return random.choice([0, 1])\n",
    "\n",
    "def generate_radio_dropdown_field_value(field):\n",
    "    # Generate synthetic data for radio and dropdown fields\n",
    "    choices_string = field['select_choices_or_calculations']\n",
    "    choices = [choice.split(',')[0].strip() for choice in choices_string.split('|')]\n",
    "    return random.choice(choices)\n",
    "\n",
    "def generate_slider_field_value(field):\n",
    "    # Generate synthetic data for slider fields\n",
    "    min_val = field['text_validation_min']\n",
    "    max_val = field['text_validation_max']\n",
    "    # If min and max are unset, 0 and 100 are assumed.\n",
    "    min_val = 0 if min_val == '' else min_val\n",
    "    max_val = 100 if max_val == '' else max_val\n",
    "    return fake.random_int(min=int(min_val), max=int(max_val))\n",
    "\n",
    "def generate_checkbox_field_value(field):\n",
    "    # Generate synthetic data for checkbox fields. More complicated due to the data structure.\n",
    "    choices_string = field['select_choices_or_calculations']\n",
    "    choices = [choice.split(', ') for choice in choices_string.split(' | ')]\n",
    "    checkbox_values = {}\n",
    "    # Randomly select which options are checked\n",
    "    for code, _ in choices:\n",
    "        if random.choice([0, 1]) == 1:\n",
    "            checkbox_values[code] = \"1\"  # Store checked options without field name prefix (will be added later, in flatten_nested_dicts())\n",
    "        else:\n",
    "            checkbox_values[code] = \"0\"  # Need to store 0 for the CSV export to work since all keys must exist across all generated records.\n",
    "    return checkbox_values\n",
    "\n",
    "def generate_synthetic_data(data_dictionary, num_records, start_num):\n",
    "    field_mapping = preprocess_data_dictionary(data_dictionary)\n",
    "    record_id_field = data_dictionary[0]['field_name']\n",
    "    records = []\n",
    "    handled_fields = []  # list to store handled field names\n",
    "    skipped_field_types = ['descriptive','sql','calc', 'file']  # List of field types to skip\n",
    "    for idx in range(num_records):\n",
    "        synthetic_record = {}\n",
    "        synthetic_record[record_id_field] = idx + start_num\n",
    "        for field_info in data_dictionary:\n",
    "            field_name = field_info['field_name']\n",
    "            field_type = field_info['field_type']\n",
    "            if field_name == record_id_field:\n",
    "                # don't generate the Record ID field with the regular method\n",
    "                continue\n",
    "            if field_type in skipped_field_types:\n",
    "                # Skip fields with types in skipped_field_types\n",
    "                continue \n",
    "            try:\n",
    "                # Associate a data synthesis method based on the field metadata\n",
    "                generate_function = field_mapping[field_name]\n",
    "                # Generate it\n",
    "                synthetic_value = generate_function(field_info)\n",
    "                # Store it in a dict\n",
    "                synthetic_record[field_name] = synthetic_value\n",
    "                # Add handled field to a list for reporting later\n",
    "                handled_fields.append(field_name) \n",
    "            except KeyError:\n",
    "                continue\n",
    "                # Anything that cannot be handled\n",
    "                print(f\"Warning: Field '{field_name}' (type '{field_type}') cannot be handled. Skipping.\")\n",
    "        # Append synthesised record to a list\n",
    "        records.append(synthetic_record)\n",
    "    \n",
    "    # Generate report for handled fields\n",
    "    report = \"Data Synthesis Report:\\n\"\n",
    "    for field_info in data_dictionary:\n",
    "        field_name = field_info['field_name']\n",
    "        if field_name == record_id_field:\n",
    "            report += f\"- {field_name}: Record ID field\\n\"\n",
    "        elif field_name in handled_fields:\n",
    "            report += f\"- {field_name}: Synthesised as field type {field_info['field_type']}\\n\"\n",
    "        elif field_info['field_type'] in skipped_field_types:\n",
    "            report += f\"- {field_name}: Field type {field_info['field_type']} skipped\\n\"\n",
    "        else:\n",
    "            report += f\"- {field_name}: Field type {field_info['field_type']} unknown\\n\"\n",
    "\n",
    "    # Return records and print report    \n",
    "    print(report)\n",
    "    return records  \n",
    "\n",
    "# Function to flatten nested dictionaries\n",
    "def flatten_nested_dicts(data_list):\n",
    "    flattened_data_list = []\n",
    "    for data in data_list:\n",
    "        flattened_data = {}\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, dict):\n",
    "                for nested_key, nested_value in value.items():\n",
    "                    flattened_key = f\"{key}___{nested_key}\"\n",
    "                    flattened_data[flattened_key] = nested_value\n",
    "            else:\n",
    "                flattened_data[key] = value\n",
    "        flattened_data_list.append(flattened_data)\n",
    "    return flattened_data_list\n",
    "\n",
    "def upload_record_to_redcap(api_url, token, record_data, forceAutoNumber):\n",
    "    payload = {\n",
    "        'token': token,\n",
    "        'content': 'record',\n",
    "        'format': 'json',\n",
    "        'type': 'flat',\n",
    "        'overwriteBehavior': 'normal',\n",
    "        'forceAutoNumber': 'true',\n",
    "        'dateFormat': 'YMD',\n",
    "        'data': record_data\n",
    "    }\n",
    "    if not forceAutoNumber:\n",
    "        payload['forceAutoNumber'] = 'false'\n",
    "    r = requests.post(api_url, data=payload)\n",
    "    if r.status_code == 200:\n",
    "        # Parse the JSON response\n",
    "        json_data = r.json()\n",
    "        return json_data\n",
    "    else:\n",
    "        raise ValueError('Error: HTTP Status ' + str(r.status_code))\n",
    "\n",
    "def export_records_to_csv(json_records, filename):\n",
    "    # Load JSON data\n",
    "    records = json.loads(json_records)\n",
    "    \n",
    "    # Check if records is empty\n",
    "    if not records:\n",
    "        print(\"No records to export.\")\n",
    "        return\n",
    "    \n",
    "    # Extract field names from the first record\n",
    "    fieldnames = records[0].keys()\n",
    "    #print(fieldnames)\n",
    "\n",
    "    # Write records to CSV file\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()  # Write header row with field names\n",
    "        writer.writerows(records)\n",
    "    return len(records)\n",
    "        \n",
    "\n",
    "def convert_dd_csv_to_json(csv_filename):\n",
    "    json_data = []\n",
    "    \n",
    "    with open(csv_filename, 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            field = {\n",
    "                'field_name': row['Variable / Field Name'],\n",
    "                'field_label': row['Field Label'],\n",
    "                'field_type': row['Field Type'],\n",
    "                'select_choices_or_calculations': row['Choices, Calculations, OR Slider Labels'],\n",
    "                'text_validation_type_or_show_slider_number': row['Text Validation Type OR Show Slider Number'],\n",
    "                'text_validation_min': row['Text Validation Min'],\n",
    "                'text_validation_max': row['Text Validation Max']\n",
    "            }\n",
    "            json_data.append(field)\n",
    "    return json_data\n",
    "\n",
    "# Main function\n",
    "def synthesise(api_url, token, dd_method='api', dd_filename=None, num_records=10, output='print', csv_filename=None, dry_run=False, forceAutoNumber=True, start_num=1):\n",
    "\n",
    "    # A bunch of defensive checks\n",
    "    assert dd_method in ['api', 'file'], \"Error: dd_method must be 'api' or 'file'.\"\n",
    "    assert output in ['print', 'csv', 'api'], \"Error: output must be 'print', 'csv' or 'api'.\"\n",
    "    assert num_records > 0, \"Error: num_records must be greater than zero.\"\n",
    "    assert type(forceAutoNumber) == bool, \"Error: forceAutoNumber must be of type boolean.\"\n",
    "    assert start_num > 0, \"Error: start_num must be greater than zero.\"\n",
    "    \n",
    "    # Collect data dictionary\n",
    "    if dd_method == 'api':\n",
    "        data_dictionary = retrieve_metadata(api_url, token)\n",
    "    elif dd_method == 'file':\n",
    "        data_dictionary = convert_dd_csv_to_json(dd_filename)\n",
    "        print(data_dictionary)\n",
    "\n",
    "    synthetic_data = generate_synthetic_data(data_dictionary, num_records, start_num)\n",
    "    flattened_data = flatten_nested_dicts(synthetic_data)\n",
    "    json_data = json.dumps(flattened_data)\n",
    "    if dry_run:\n",
    "            print(\"Dry run. No data generated.\")\n",
    "    elif output == 'api':\n",
    "        project_info = test_connection(api_url, token, silent=True)\n",
    "        if project_info['in_production'] == 1:\n",
    "            print(\"This project is in production. Cannot import synthetic data.\")\n",
    "            return\n",
    "        try:\n",
    "            response = upload_record_to_redcap(api_url, token, json_data, forceAutoNumber=forceAutoNumber)\n",
    "            print(f\"Success! Created {response['count']} records and imported to REDCap.\")\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "    elif output == 'csv':\n",
    "        csv_count = export_records_to_csv(json_data,csv_filename)\n",
    "        print(f\"Success! Created {csv_count} records and saved as {csv_filename}.\")\n",
    "    elif output == 'print':\n",
    "        print(f\"Success! Created {len(flattened_data)} records.\")\n",
    "        print(json.dumps(flattened_data, indent=4))\n",
    "\n",
    "def test_connection(api_url, token, silent=False):\n",
    "    data = {\n",
    "        'token': token,\n",
    "        'content': 'project',\n",
    "        'format': 'json',\n",
    "        'returnFormat': 'json'\n",
    "    }\n",
    "    r = requests.post(api_url,data=data)\n",
    "    if str(r.status_code)[0] == '2':\n",
    "        if not silent:\n",
    "            print(\"Connection successful!\")\n",
    "            print(\"Project details:\")\n",
    "            print(r.json())\n",
    "    else:\n",
    "        print('Could not connect.')\n",
    "        print('HTTP Status: ' + str(r.status_code))\n",
    "    if silent:\n",
    "        return r.json()\n",
    "    \n",
    "# Todo: \n",
    "# Longitudinal\n",
    "# Repeating\n",
    "# Selective fields/forms/events\n",
    "# fuzzily determine fake method to use for text fields based on label/name (like ssn)\n",
    "# Allow for custom map overrides in an easy way\n",
    "# Allow for custom regex patterns, might be complex!\n",
    "# Parse normal variation modelling from data dictionary or even download and aggregate real data for generating test data with.\n",
    "# Separate tokens for retrieve DD, build model, import data.\n",
    "# Handle API call errors better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874792fe-b0f9-4ba0-9af0-c3e317a2c177",
   "metadata": {},
   "source": [
    "## Define variables\n",
    "\n",
    "Complete the below variables and run the cell to commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ee5d28-1555-4872-8ce5-b7c8f94eada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = 'https://redcap.example.com/api/'  # Enter your API URL\n",
    "token = 'TOKEN'                              # Enter your API token\n",
    "dd_method='api'                              # Method to retrieve data dictionary. 'api' (Default): retrieve data dictionary via api. 'file': Supply a data dictionary as csv using the <dd_filename> variable.\n",
    "dd_filename=None                             # Filename (and path) to data dictionary CSV (Default: None)\n",
    "num_records=1                                # Number of records to generate (Default: 10)\n",
    "output='print'                               # 'print' (Defult): Print record data as output. 'csv': Save record data as CSV (with filename supplied as <csv_filename>), 'api': Import to REDCap via API.\n",
    "csv_filename=None                            # Filename (and path) for output CSV (if <output> = 'csv')\n",
    "dry_run=False                                # If True, do not export records; just build model and output report. (Default: False)\n",
    "email_domain = \"example.com\"                 # Email domain used to generate emails (Default: 'example.com')\n",
    "forceAutoNumber=True                         # If True, all records will be renamed (new records created). If False, record ID field's value will be used to select record to update. (Default: True)\n",
    "start_num=1                                  # First number for record ID field value (Default: 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025474bc-39b1-43ab-8b97-6a1cfadf5706",
   "metadata": {},
   "source": [
    "## Test connection\n",
    "\n",
    "If successful, will print project information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073e1581-df4c-4d85-adcf-e079995a2133",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_connection(api_url=api_url, token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5bb2d9-6e60-4d5d-9b3b-ed8c5b7d17be",
   "metadata": {},
   "source": [
    "## Synthesise Data\n",
    "\n",
    "With the above configuration saved, execute the `synthesise()` function. With the defaults, the generated records will not be imported, but printed as output in this notebook. If you are happy with the results, set `output` to either `csv` or `api`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8607c630-d50d-4651-9fcb-06405879d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesise(api_url, token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
